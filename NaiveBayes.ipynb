{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "import os\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix,recall_score,f1_score,precision_score,accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_feature(path):\n",
    "    df=pd.read_csv(path,sep=\"\\t\",header=None)\n",
    "    sentences=df[3]\n",
    "    label=df[1].values\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "\n",
    "    vocab=open(\"outputs/SST2/vocab.txt\",encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "    vocab_len=len(vocab)\n",
    "    sen_len=len(sentences)\n",
    "    \n",
    "    x=[]\n",
    "    for c,i in enumerate(sentences):\n",
    "        tokens=tokenizer.tokenize(i)\n",
    "        ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "        tx=np.array([0]*vocab_len)\n",
    "        tx[ids]=1\n",
    "        x.append(tx)\n",
    "        print(round(c/sen_len,3),end=\"\\r\")\n",
    "    x=np.array(x)\n",
    "    return x,label\n",
    "\n",
    "\n",
    "def get_eval_report(task_name, labels, preds):\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    rec=recall_score(labels,preds)\n",
    "    f1=f1_score(labels,preds)\n",
    "    prec=precision_score(labels,preds)\n",
    "    acc=accuracy_score(labels,preds)\n",
    "    return {\n",
    "        \"task\": task_name,\n",
    "        \"mcc\": mcc,\n",
    "        \"recall\":rec,\n",
    "        \"precision\":prec,\n",
    "        \"F1\":f1,\n",
    "        \"accuracy\":acc,\n",
    "        \"tp\": tp,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn\n",
    "    }\n",
    "\n",
    "def compute_metrics(task_name, labels, preds):\n",
    "    assert len(preds) == len(labels)\n",
    "    return get_eval_report(task_name, labels, preds)\n",
    "\n",
    "def show(dic):\n",
    "    for i in dic:\n",
    "        if i==\"task\":\n",
    "            print(\"%10s:%35s\"%(i,dic[i]))\n",
    "        else:\n",
    "            print(\"%10s:%35.3f\"%(i,dic[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    if os.path.exists(\"data/train_x.npy\"):\n",
    "        train_x=np.load(\"data/train_x.npy\")\n",
    "        train_y=np.load(\"data/train_y.npy\")\n",
    "        test_x=np.load(\"data/test_x.npy\")\n",
    "        test_y=np.load(\"data/test_y.npy\")\n",
    "    else:\n",
    "        train_x,train_y=text_to_feature(\"data/train.tsv\")\n",
    "        test_x,test_y=text_to_feature(\"data/dev.tsv\")\n",
    "    return train_x,train_y,test_x,test_y\n",
    "    \n",
    "    \n",
    "    \n",
    "train_x,train_y,test_x,test_y=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13470, 28996)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=gnb.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dic=compute_metrics(\"Naive Bayes Sentiment Analysis\",test_y,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      task:     Naive Bayes Sentiment Analysis\n",
      "       mcc:                              0.576\n",
      "    recall:                              0.664\n",
      " precision:                              0.898\n",
      "        F1:                              0.764\n",
      "  accuracy:                              0.772\n",
      "        tp:                           4977.000\n",
      "        tn:                           5416.000\n",
      "        fp:                            564.000\n",
      "        fn:                           2513.000\n"
     ]
    }
   ],
   "source": [
    "show(result_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
